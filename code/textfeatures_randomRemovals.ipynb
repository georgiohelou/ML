{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso\n",
    "from page import pagerank\n",
    "# from Bert_function import author_embedding\n",
    "from deepwalk_generate import DpWalk\n",
    "from processing_abstract import process_abstracts\n",
    "from process_authorFile import process_authorFiles\n",
    "from final_dico_creation import dictionary_concatenation\n",
    "from final_dico_creation import dictionary_concatenation2  # keeps abstracts separated\n",
    "from Text_Embedding2 import Embed_Author\n",
    "import pickle\n",
    "import copy\n",
    "from MLP import prepare_data, MLP, train_model, evaluate_model, predict\n",
    "import nltk\n",
    "print(\"Imported all libraries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run ##\n",
    "print(\"Loading & printing DictForAuthor_new.pkl\")\n",
    "infile = open('DictForAuthor_new.pkl', 'rb')\n",
    "DictForAuthor_new = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "print(\"type(DictForAuthor_new) : \", type(DictForAuthor_new))\n",
    "print(\"len(DictForAuthor_new) = \", len(DictForAuthor_new))\n",
    "# print(\"sentences[0] : \\n\", sentences[0])\n",
    "print(\"DictForAuthor_new.pkl is compromised. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run ##\n",
    "''' \n",
    "To generate a dictionary matching the authors to their concatenated abstracts [dico in which keys=author_IDs & values=concatenated abstracts as a string].\n",
    "Dictionary name: DictForAuthor\n",
    "Dictionary saved as: authors_with_all_abstracts.pkl\n",
    "'''\n",
    "# print(\"extracting abstracts\")\n",
    "# DictOfAbstracts = {}\n",
    "# infile = open('abstracts_data.pkl', 'rb')\n",
    "# DictOfAbstracts = pickle.load(infile)\n",
    "# infile.close()\n",
    "# print(\"extracting authors-paperIDs [dico]\")\n",
    "# DictOfPaperID = process_authorFiles()\n",
    "# # print(DictOfPaperID['2318996053']) # works\n",
    "# print(\"matching authors to all of their abstracts [dico]\")\n",
    "# DictForAuthor = dictionary_concatenation(DictOfAbstracts, DictOfPaperID)\n",
    "# # print(DictForAuthor['2318996053']) # works\n",
    "# with open('authors_with_all_abstracts.pkl', 'wb') as f:\n",
    "#     pickle.dump(DictForAuthor, f)\n",
    "# f.close()\n",
    "# print(\"Operation completed. Check authors_with_all_abstracts.pkl file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run ##\n",
    "''' \n",
    "To generate a dictionary matching the authors to their *separated* abstracts [dico in which keys=author_IDs & values=a list with the separated abstracts].\n",
    "Dictionary name: DictForAuthor2\n",
    "Dictionary saved as: authors_with_all_abstracts_separated.pkl\n",
    "'''\n",
    "print(\"extracting abstracts\")\n",
    "DictOfAbstracts = {}\n",
    "infile = open('abstracts_data.pkl', 'rb')\n",
    "DictOfAbstracts = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "print(\"extracting authors-paperIDs [dico]\")\n",
    "DictOfPaperID = process_authorFiles()\n",
    "print(DictOfPaperID['2318996053'])\n",
    "\n",
    "print(\"matching authors to all of their abstracts *separated* [dico-list]\")\n",
    "DictForAuthor = dictionary_concatenation2(DictOfAbstracts, DictOfPaperID) \n",
    "print(DictForAuthor['2318996053'])\n",
    "print(\"len(DictForAuthor['2318996053']) = \", len(DictForAuthor['2318996053']))\n",
    "\n",
    "with open('authors_with_all_abstracts_separated.pkl', 'wb') as f:\n",
    "    pickle.dump(DictForAuthor, f)\n",
    "f.close()\n",
    "print(\"Operation completed. Check authors_with_all_abstracts_separated.pkl file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run ##\n",
    "# # takes care of cleaning the stringremove stopwords\n",
    "''' \n",
    "To generate a dictionary matching the authors to their *separated & cleaned* abstracts \n",
    "[dico in which keys=author_IDs & values=a list with the separated abstracts cleaned and w/o stopwords].\n",
    "Dictionary name: DictForAuthor_cleaned\n",
    "Dictionary saved as: authors_with_all_abstracts_separated_cleaned.pkl\n",
    "'''\n",
    "DictForAuthor_cleaned = DictForAuthor # make copy\n",
    "specialChars = \".,()\"\n",
    "specialChars2 = ['\\r', '\\n', '\\t', '\\\\r', '\\\\n', '\\\\t', '\\\\r\\\\n\\\\r\\\\n', '\\\\r\\\\n\\\\r\\\\n']\n",
    "stop_words = {'ourselves', 'hers', 'between', 'yourself', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'itself', 'other', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while',\n",
    "              'above', 'up', 'to', 'ours', 'had', 'she', 'when', 'at', 'any', 'before', 'them', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "\n",
    "for authorID in tqdm(DictForAuthor_cleaned): # iterate over dictionary\n",
    "    for i in range(len(DictForAuthor_cleaned[authorID])): # iterate over the author's separate abstracts\n",
    "        abstractText = str(DictForAuthor_cleaned[authorID][i])\n",
    "        ## clean string and remove stopwords ##\n",
    "        for specialChar in specialChars:\n",
    "            abstractText = abstractText.replace(specialChar, \"\")\n",
    "        for specialChar2 in specialChars2:\n",
    "            abstractText = abstractText.replace(specialChar2, \"\")\n",
    "        abstractText = abstractText.replace(\"   \", \" \")\n",
    "        abstractText = abstractText.replace(\"  \", \" \")\n",
    "        abstractText = abstractText.strip()\n",
    "        abstractText_words = abstractText.split()  # isolate words\n",
    "        for j in range(len(abstractText_words)):\n",
    "            if abstractText_words[j] in stop_words:\n",
    "                abstractText_words[j] = \"\"\n",
    "        # for stop_word in stop_words:\n",
    "        #     abstractText = abstractText.replace(stop_word, \"\")\n",
    "\n",
    "        temp = ' '.join(abstractText_words)\n",
    "        temp = temp.strip()\n",
    "        temp = temp.replace(\"  \", \" \")\n",
    "        temp = temp.replace(\"   \", \" \")\n",
    "        DictForAuthor_cleaned[authorID][i] = ' '.join(temp.split())\n",
    "\n",
    "with open('authors_with_all_abstracts_separated_cleaned.pkl', 'wb') as f:\n",
    "    pickle.dump(DictForAuthor_cleaned, f)\n",
    "f.close()\n",
    "print(\"Operation completed. Check authors_with_all_abstracts_separated_cleaned.pkl file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can continue here ---------- directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217801\n"
     ]
    }
   ],
   "source": [
    "# authors_with_all_abstracts_separated_cleaned.pkl is GOOD.\n",
    "DicoCleaned = {}\n",
    "infile = open('authors_with_all_abstracts_separated_cleaned.pkl', 'rb')\n",
    "DicoCleaned = pickle.load(infile)\n",
    "infile.close()\n",
    "print(len(DicoCleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The present\n",
      "1036332\n",
      "['An underground utility conveyance 10 may precisely located impressing both locating tone confirmation tone conveyance The locating tone RF signal produces first field radiates earth sensed first above-ground signal detector 22 establish general location conveyance The confirmation tone near-DC signal produces second field relatively limited radiation path A second detector 24 inserted earth successive places region conveyance located establish precise location', 'The invention relates method system wireless network based mapping method system supporting field personnel access select download facilities related maps related information Mapping files manipulated accordance grid block overlay first level map display Each grid blocks uniquely identifiable indexed one algorithms necessary local creation enhanced and/or enlarged second level display', 'The present invention system method searching large database dig location tickets tickets particular interest A user locate area concern on-screen map view list tickets received given date range map For example underground plant damage occurs particular region users need text-search database locate particular ticket may caused damage but instead locate ticket using enhanced on-screen map invention', 'A method apparatus calculating engineered capacity packet network described In one example subscriber usage data collected packet network The engineered capacity updated using usage data periodic basis In another example engineered capacity subsequently compared actual network usage Afterwards alarm triggered actual network usage reaches predefined threshold engineered capacity', 'A method apparatus detecting abnormal calling activity communications network described In one embodiment usage data associated least one phone number obtained communications network The usage data subsequently processed determine abnormal calling activity associated least one phone number exhibited']\n"
     ]
    }
   ],
   "source": [
    "# checkClean = []\n",
    "for author in DicoCleaned:\n",
    "    allAbstracts = ' '.join(DicoCleaned[author])\n",
    "    checkClean = allAbstracts.split()\n",
    "    if \"The\" in checkClean:\n",
    "        print(\"The present\")\n",
    "        print(author)\n",
    "        print(DicoCleaned[author])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036332\n",
      "1101850\n",
      "1336878\n",
      "1515524\n",
      "1606427\n",
      "2728936\n",
      "3720290\n",
      "4265922\n",
      "4631324\n",
      "4652436\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for author in DicoCleaned:\n",
    "    print(author)\n",
    "    count +=1\n",
    "    if count == 10:\n",
    "        break\n",
    "## order is preserved (i.e. same as in author_papers.txt) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing batch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (736) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jj/16td7hns7hz91ykyd37c07mw0000gn/T/ipykernel_7771/617709567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample1_BERTembedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbed_Author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample1_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/Text_Embedding2.py\u001b[0m in \u001b[0;36mEmbed_Author\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mAllVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/Text_Embedding2.py\u001b[0m in \u001b[0;36mbert\u001b[0;34m(self, sentences, pretrained_weights)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         return self.transformer(\n\u001b[1;32m    552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (736) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "example1_BERTembedding = Embed_Author(example1_concat)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random method used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217801/217801 [00:23<00:00, 9230.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors-abstracts reduced:  85328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## do not run ##\n",
    "def process_smart(dico_inner, method, minNumWords=450):\n",
    "    numOps = 0\n",
    "    new_dico = {}\n",
    "    if method == \"endOf\":\n",
    "        for authorID in tqdm(dico_inner):\n",
    "            # print(authorID)\n",
    "            abstractConcat = ' '.join(dico_inner[authorID])\n",
    "            numAbstracts = len(dico_inner[authorID]); abstractLength = len(abstractConcat); numWords = len(abstractConcat.split())\n",
    "            # print(\"numAbstracts : \", numAbstracts); print(\"abstractLength : \", abstractLength); print(\"numWords : \", numWords)\n",
    "            if numWords < minNumWords:\n",
    "                continue\n",
    "            numOps += 1\n",
    "            diff = numWords - minNumWords; numRemovalsPerAbstract = diff // numAbstracts; numAdditionalRemovals = diff % numAbstracts\n",
    "            # print(\"numRemovalsPerAbstract : \", numRemovalsPerAbstract); print(\"numAdditionalRemovals : \", numAdditionalRemovals)\n",
    "            for i in range(numAbstracts): ## for each abstract rmv words\n",
    "                abstract = dico_inner[authorID][i]; abstractWords = abstract.split(); size = len(abstractWords); \n",
    "                abstractWords = abstractWords[:size-numRemovalsPerAbstract]; dico_inner[authorID][i] = ' '.join(abstractWords)\n",
    "            for j in range(numAdditionalRemovals):\n",
    "                abstract = dico_inner[authorID][numAbstracts-1-j]; abstractWords = abstract.split(); size = len(abstractWords); \n",
    "                abstractWords = abstractWords[:size-1]; dico_inner[authorID][numAbstracts-1-j] = ' '.join(abstractWords)\n",
    "\n",
    "            abstractConcat = ' '.join(dico_inner[authorID])\n",
    "            abstractLength = len(abstractConcat)\n",
    "            numWords = len(abstractConcat.split())\n",
    "            # print(\"abstractLength : \", abstractLength); print(\"numWords : \", numWords)\n",
    "            new_dico = dico_inner\n",
    "\n",
    "    if method == \"random\":\n",
    "        print(\"random method used\")\n",
    "        for authorID in tqdm(dico_inner):\n",
    "            abstractConcat = ' '.join(dico_inner[authorID])\n",
    "            abstractConcatWord = abstractConcat.split(); numWords = len(abstractConcatWord)\n",
    "            if numWords <= minNumWords:\n",
    "                new_dico[authorID] = DicoCleaned[authorID]\n",
    "                continue\n",
    "            else:\n",
    "                numOps += 1\n",
    "                while(numWords > minNumWords):\n",
    "                    rmv_index = random.randint(0, numWords-1)\n",
    "                    del abstractConcatWord[rmv_index]\n",
    "                    numWords -= 1\n",
    "                new_dico[authorID] = ' '.join(abstractConcatWord)\n",
    "\n",
    "    print(\"Number of authors-abstracts reduced: \", numOps)\n",
    "\n",
    "    return new_dico\n",
    "\n",
    "\n",
    "\n",
    "# dico2 = dicoGame.copy()\n",
    "dico_new = process_smart(DicoCleaned, \"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do not run ##\n",
    "## works/good##\n",
    "sentences_cleaned = []\n",
    "for author in DicoCleaned:\n",
    "    sentences_cleaned.append(' '.join(DicoCleaned[author]))\n",
    "\n",
    "textfile = open(\"sentences_cleaned.txt\", \"w\")\n",
    "for element in sentences_cleaned:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217801\n",
      "random method used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217801/217801 [00:55<00:00, 3927.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(DicoCleaned))\n",
    "print(\"random method used\")\n",
    "minNumWords = 450\n",
    "new_dico = {}\n",
    "for authorID in tqdm(DicoCleaned):\n",
    "    abstractConcat = ' '.join(DicoCleaned[authorID])\n",
    "    abstractConcatWord = abstractConcat.split()\n",
    "    numWords = len(abstractConcatWord)\n",
    "    # print(numWords)\n",
    "    \n",
    "    if numWords <= minNumWords:\n",
    "        new_dico[authorID] = ' '.join(abstractConcatWord)\n",
    "        continue\n",
    "    else:\n",
    "        while(numWords > minNumWords):\n",
    "            rmv_index = random.randint(0, numWords-1)\n",
    "            del abstractConcatWord[rmv_index]\n",
    "            numWords -= 1\n",
    "        # print(\"' '.join(abstractConcatWord) : \\n\", ' '.join(abstractConcatWord), \"\\n\")\n",
    "        new_dico[authorID] = ' '.join(abstractConcatWord)\n",
    "        if len(new_dico[authorID].split()) > minNumWords:\n",
    "            print(\"WOW\")\n",
    "            print(authorID)\n",
    "            break\n",
    "\n",
    "print(len(new_dico))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in new_dico:\n",
    "    abstr = new_dico[author].split()\n",
    "    # print(abstr)\n",
    "    if len(abstr) > 450:\n",
    "        print(\"GODDAMN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation completed. Check authors_with_all_abstracts_random_max450words.pkl file.\n"
     ]
    }
   ],
   "source": [
    "with open('authors_with_all_abstracts_random_max450words.pkl', 'wb') as f:\n",
    "    pickle.dump(new_dico, f)\n",
    "f.close()\n",
    "print(\"Operation completed. Check authors_with_all_abstracts_random_max450words.pkl file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### worked ###\n",
    "sentences_cleaned_450words = [] \n",
    "for author in new_dico:\n",
    "    sentences_cleaned_450words.append(new_dico[author])\n",
    "\n",
    "with open('sentences_cleaned_random_max450words.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences_cleaned_450words, f)\n",
    "f.close()\n",
    "\n",
    "# textfile = open(\"sentences_cleaned_random_max450words.txt\", \"w\")\n",
    "# for element in sentences_cleaned_450words:\n",
    "#     textfile.write(element + \"\\n\")\n",
    "# textfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217801\n",
      "mobile cloud rapidly terms use case functionalityBoth two rapidly case This reviews current energy consumption cloud computing proposes system whereby applications may profiled consumption augmentation required negotiate external optimum Such system useful resources may computing functionality location computing provisioned both end users These two fields order greater functionality mobile devices number different ways Augmentation resources cloud shown one energy power mobile devices due resource constrained communication interfaces often fine line offloading resources The internet things IoT current state large number connected huge amount data needs lot operations useful information control data requires design accelerate enhance The defined new paradigm appeared hide all complexity system all controls management devices things inside middleware layer software In work defined model simplify IoT management process vital solution challenges architecture store produced IoT software defined network defined software defined security defined model Cloud Computing MCC viable solution battery lifetime storage capacity using processing intensive mobile jobs take place cloud system results mobile This reduce time completing jobs However connecting cloud network latency huge consumption 3G/LTE connections introduce aiming power consumption concepts Cloudlet framework new model practical experimental results showed using reduces reducing communication latency mobile requests high service stander Cloud computing fast-growing gained great industry Consequently researchers actively cloud computing research projects One major cloud computing researchers lack cloud use studies simulation environment used wide cloud processing elements data storage Service Level constraints applications Oriented Architecture management automation Management BPM introduces Rain generator emulates cloud Also MapReduce processing model CloudExp order handle data problems evolving fast-growing computing great both industry universities actively cloud IT curricula challenge facing cloud computing instructors lack tool experiment This paper TeachCloud simulation used experiment cloud components as: processing elements data centres service level agreement constraints applications management process Also TeachCloud MapReduce handle embarrassingly data problems TeachCloud CloudSim research-oriented cloud computing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "infile = open('sentences_cleaned_random_max450words.pkl', 'rb')\n",
    "sentencesX = pickle.load(infile)\n",
    "infile.close()\n",
    "print(len(sentencesX))\n",
    "# print(len(new_dico))\n",
    "\n",
    "print(sentencesX[20])\n",
    "print()\n",
    "# print(new_dico['7769909'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "Vectorizing batch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (700) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jj/16td7hns7hz91ykyd37c07mw0000gn/T/ipykernel_2779/456485924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentencesX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mBERTembedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbed_Author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/Text_Embedding2.py\u001b[0m in \u001b[0;36mEmbed_Author\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mAllVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/Text_Embedding2.py\u001b[0m in \u001b[0;36mbert\u001b[0;34m(self, sentences, pretrained_weights)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         return self.transformer(\n\u001b[1;32m    552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XParis/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (700) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "subset = sentencesX[0:6000]\n",
    "print(len(subset))\n",
    "BERTembedding = Embed_Author(subset)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The increasing pervasiveness location-acquisition technologies GPS GSM networks etc leading collection large spatio-temporal datasets opportunity discovering usable knowledge movement behaviour fosters novel applications services In paper move towards direction develop extension sequential pattern mining paradigm analyzes trajectories moving objects We introduce trajectory patterns concise descriptions frequent behaviours terms both space ie regions space visited movements time ie duration movements In setting provide general formal statement novel mining problem study several different instantiations different complexity The various approaches empirically evaluated real data synthetic benchmarks comparing strengths weaknesses', 'Our understanding individual mobility patterns shape impact social network limited but essential deeper understanding network dynamics evolution This question largely unexplored partly due difficulty obtaining large-scale society-wide data simultaneously capture dynamical information individual movements social interactions Here address challenge first time tracking trajectories communication records 6 Million mobile phone users We find similarity two individuals\\\\u0027 movements strongly correlates proximity social network We investigate predictive power hidden correlations exploited address challenging problem: new links develop social network We show mobility measures alone yield surprising predictive power comparable traditional network-based measures Furthermore prediction accuracy significantly improved learning supervised classifier based combined mobility network measures We believe findings interplay mobility patterns social ties offer new perspectives link prediction but also network dynamics', 'Spatio-temporal geo-referenced datasets growing rapidly near future due both technological social/commercial reasons From data mining viewpoint spatio-temporal trajectory data introduce new dimensions correspondingly novel issues performing analysis tasks In paper consider clustering problem applied trajectory data domain In particular propose adaptation density-based clustering algorithm trajectory data based simple notion distance trajectories Then set experiments synthesized data performed order test algorithm compare standard clustering approaches Finally new approach trajectory clustering problem called temporal focussing sketched aim exploiting intrinsic semantics temporal dimension improve quality trajectory clustering', 'The technologies mobile communications pervade society wireless networks sense movement people generating large volumes mobility data mobile phone call records Global Positioning System GPS tracks In work illustrate striking analytical power massive collections trajectory data unveiling complexity human mobility We present results large-scale experiment based detailed trajectories tens thousands private cars on-board GPS receivers tracked weeks ordinary mobile activity We illustrate knowledge discovery process based data addresses fundamental questions mobility analysts: frequent patterns people\\\\u0027s travels? How big attractors extraordinary events influence mobility? How predict areas dense traffic near future? How characterize traffic jams congestions? We also describe M-Atlas querying mining language system makes analytical process possible providing mechanisms master complexity transforming raw GPS tracks mobility knowledge M-Atlas centered onto concept trajectory mobility knowledge discovery process specified M-Atlas queries realize data transformations data-driven estimation parameters mining methods quality assessment obtained results quantitative visual exploration discovered behavioral patterns models composition mined patterns models data analyses mining incremental mining strategies address scalability']\n"
     ]
    }
   ],
   "source": [
    "infile = open('authors_with_all_abstracts_separated_cleaned.pkl', 'rb')\n",
    "dicoY = pickle.load(infile)\n",
    "infile.close()\n",
    "print(dicoY['7769909'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "infile = open('fullEmbeddings_random.pkl', 'rb')\n",
    "fullEmbeddings_random = pickle.load(infile)\n",
    "infile.close()\n",
    "print(len(fullEmbeddings_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaf3190ec0cb36108f7b7c7b298c2782f64ad086d0ec4d49ed3edd66dd7efc4f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('XParis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
